# Quiz: Stochastic Processes and Random Signals

Test your understanding of random processes, noise characterization, power spectral density, and statistical signal processing.

---

#### 1. What is a random process (stochastic process)?

<div class="upper-alpha" markdown>
1. A deterministic signal that can be predicted exactly
2. A collection of random variables indexed by time, representing signals whose exact values cannot be predicted but whose statistical properties can be characterized
3. Any signal corrupted by measurement noise
4. A periodic signal with random phase
</div>

??? question "Show Answer"
    The correct answer is **B**. A random process (also called a stochastic process) is a collection of random variables indexed by time, representing signals whose exact values cannot be predicted but whose statistical properties can be characterized. Each time instance $t$ corresponds to a random variable $X(t)$ with an associated probability distribution. Random processes are characterized through ensemble statistics including mean function, autocorrelation function, and autocovariance function.

    **Concept Tested:** Random Processes, Stochastic Signals

    **See:** [Random Processes and Stochastic Signals](index.md#random-processes-and-stochastic-signals)

---

#### 2. What are the two requirements for a random process to be wide-sense stationary (WSS)?

<div class="upper-alpha" markdown>
1. Zero mean and unit variance
2. Constant mean and autocorrelation depending only on time difference
3. Gaussian distribution and finite variance
4. Periodic mean and periodic autocorrelation
</div>

??? question "Show Answer"
    The correct answer is **B**. A wide-sense stationary (WSS) process requires: (1) constant mean $\mu_X(t) = \mu_X$ for all $t$, and (2) autocorrelation depending only on time difference: $R_X(t_1, t_2) = R_X(\tau)$ where $\tau = t_2 - t_1$. WSS processes simplify analysis significantly because their correlation structure depends only on time lag rather than absolute time. Most naturally occurring noise sources approximate WSS conditions over reasonable time scales.

    **Concept Tested:** Random Processes, Stochastic Signals

    **See:** [Stationarity](index.md#stationarity)

---

#### 3. What characterizes white noise in both time and frequency domains?

<div class="upper-alpha" markdown>
1. Gaussian probability distribution with zero mean
2. Delta function autocorrelation $R_w[k] = \sigma^2 \delta[k]$ and flat power spectral density $S_w(f) = \sigma^2$
3. Constant amplitude across all time samples
4. Zero variance and non-zero mean
</div>

??? question "Show Answer"
    The correct answer is **B**. White noise has delta function autocorrelation $R_w[k] = \sigma^2 \delta[k]$, meaning samples are completely uncorrelated, and constant (flat) power spectral density $S_w(f) = \sigma^2$ across all frequencies. The name "white" comes from the analogy to white light containing all wavelengths equally. This means equal power at all frequencies and zero correlation between samples: $E[w[n]w[m]] = 0$ for $n \neq m$.

    **Concept Tested:** White Noise

    **See:** [White Noise](index.md#white-noise)

---

#### 4. How does colored noise differ from white noise?

<div class="upper-alpha" markdown>
1. Colored noise has imaginary components while white noise is real
2. Colored noise has non-uniform power spectral density with power concentrated in specific frequency ranges
3. Colored noise always has higher power than white noise
4. Colored noise can only be generated synthetically
</div>

??? question "Show Answer"
    The correct answer is **B**. Colored noise has non-uniform power spectral density, with power concentrated in specific frequency ranges, unlike white noise which has flat spectrum. The autocorrelation function of colored noise is not a delta function, indicating temporal correlation between samples. Common types include pink noise ($1/f$, equal power per octave) and brown noise ($1/f^2$, integrated white noise). Colored noise can be generated by filtering white noise through appropriately designed systems.

    **Concept Tested:** Colored Noise

    **See:** [Colored Noise](index.md#colored-noise)

---

#### 5. Why are Gaussian random processes particularly important in signal processing?

<div class="upper-alpha" markdown>
1. They are the only type of noise that exists in nature
2. They are completely characterized by mean and covariance, and arise naturally via the central limit theorem
3. They have zero variance
4. They cannot be filtered
</div>

??? question "Show Answer"
    The correct answer is **B**. Gaussian processes are completely characterized by mean and covariance (all higher-order statistics are determined by these), and arise naturally through the central limit theorem: sums of many independent random variables approach Gaussian distribution. Additional important properties include: linear systems preserve Gaussianity, many calculations have closed-form solutions, and they provide tractable mathematical models. The central limit theorem explains prevalence since many noise sources sum numerous independent factors.

    **Concept Tested:** Gaussian Noise

    **See:** [Properties of Gaussian Processes](index.md#properties-of-gaussian-processes)

---

#### 6. What does signal-to-noise ratio (SNR) quantify?

<div class="upper-alpha" markdown>
1. The sampling rate relative to bandwidth
2. The relative strength of desired signal to background noise, fundamentally limiting system performance
3. The number of quantization levels in an ADC
4. The filter order required for noise removal
</div>

??? question "Show Answer"
    The correct answer is **B**. Signal-to-noise ratio (SNR) quantifies the relative strength of desired signal to background noise: $\text{SNR} = P_{signal}/P_{noise} = \sigma_s^2/\sigma_n^2$ (power ratio) or $\text{SNR}_{dB} = 10\log_{10}(P_{signal}/P_{noise})$ (decibel scale). SNR fundamentally limits system performance in detection, estimation, and communication applications. Shannon's capacity theorem shows channel capacity depends directly on SNR: $C = B\log_2(1 + \text{SNR})$.

    **Concept Tested:** Signal-to-Noise Ratio

    **See:** [Signal-to-Noise Ratio](index.md#signal-to-noise-ratio)

---

#### 7. How does time-domain averaging reduce noise for repetitive signals?

<div class="upper-alpha" markdown>
1. It increases signal power while keeping noise power constant
2. It keeps signal unchanged while reducing noise variance by factor $K$ for $K$ independent observations
3. It eliminates all noise completely
4. It converts colored noise to white noise
</div>

??? question "Show Answer"
    The correct answer is **B**. For repetitive signals with uncorrelated noise, averaging $K$ independent observations $\hat{s}[n] = \frac{1}{K}\sum_{i=1}^K x_i[n]$ keeps the signal component unchanged while reducing noise variance to $\sigma_{noise}^2 = \sigma_w^2/K$, improving SNR by factor $K$ (or $10\log_{10}K$ dB). This technique is fundamental in medical imaging (evoked potentials), signal averaging oscilloscopes, and any application where the signal repeats but noise remains uncorrelated across realizations.

    **Concept Tested:** Noise Reduction

    **See:** [Time-Domain Averaging](index.md#time-domain-averaging)

---

#### 8. What is the Wiener-Khinchin theorem?

<div class="upper-alpha" markdown>
1. It proves that all signals are bandlimited
2. It establishes that autocorrelation and power spectral density form a Fourier transform pair
3. It describes the sampling theorem
4. It defines the Z-transform
</div>

??? question "Show Answer"
    The correct answer is **B**. The Wiener-Khinchin theorem establishes the fundamental relationship that for wide-sense stationary processes, the autocorrelation function and power spectral density form a Fourier transform pair: $S_X(f) = \mathcal{F}\{R_X(\tau)\}$ and $R_X(\tau) = \mathcal{F}^{-1}\{S_X(f)\}$. This unifies time-domain correlation analysis with frequency-domain spectral analysis, enabling computation of one from the other and bridging correlation and spectrum concepts.

    **Concept Tested:** Wiener-Khinchin Theorem

    **See:** [Wiener-Khinchin Theorem](index.md#wiener-khinchin-theorem)

---

#### 9. What does the power spectral density (PSD) describe?

<div class="upper-alpha" markdown>
1. The time-varying power of a signal
2. How signal power is distributed across frequency for random processes
3. The phase spectrum of a signal
4. The impulse response of a filter
</div>

??? question "Show Answer"
    The correct answer is **B**. The power spectral density (PSD) describes how signal power distributes across the frequency spectrum for random processes. For a WSS process with autocorrelation $R_X(\tau)$, the PSD is the Fourier transform: $S_X(f) = \int_{-\infty}^{\infty} R_X(\tau)e^{-j2\pi f\tau}d\tau$. Integration of the PSD over all frequencies yields total signal power. For linear systems, output PSD relates to input PSD through $S_y(f) = |H(f)|^2 S_x(f)$.

    **Concept Tested:** Power Spectral Density

    **See:** [Power Spectral Density](index.md#power-spectral-density)

---

#### 10. What is the primary disadvantage of the periodogram method for PSD estimation?

<div class="upper-alpha" markdown>
1. It requires too much computational power
2. It is an inconsistent estimator with variance that does not decrease as data length increases
3. It only works with white noise
4. It produces biased estimates of the mean
</div>

??? question "Show Answer"
    The correct answer is **B**. The periodogram $\hat{S}_X(f) = \frac{1}{N}|X(f)|^2$ is an inconsistent estimatorâ€”its variance does not decrease with increased data length, remaining proportional to the true PSD squared. This makes it unreliable for single realizations. Welch's method improves periodogram estimation by dividing data into overlapping segments, windowing each, computing periodograms, and averaging, trading frequency resolution for reduced variance through averaging.

    **Concept Tested:** Power Spectral Density, Statistical Signal Processing

    **See:** [PSD Estimation](index.md#psd-estimation)

---
